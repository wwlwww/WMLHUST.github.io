{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://yoursite.com","root":"/"},"pages":[{"title":"categories","date":"2020-05-11T09:14:14.000Z","updated":"2020-05-11T09:14:36.963Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2020-05-11T09:20:22.000Z","updated":"2020-05-11T09:22:25.642Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"about about"},{"title":"tags","date":"2020-05-11T09:10:36.000Z","updated":"2020-05-11T09:11:52.492Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"常见web安全总结","slug":"常见web安全总结","date":"2020-05-14T08:20:55.000Z","updated":"2020-05-14T08:20:55.000Z","comments":true,"path":"2020/05/14/常见web安全总结/","link":"","permalink":"http://yoursite.com/2020/05/14/%E5%B8%B8%E8%A7%81web%E5%AE%89%E5%85%A8%E6%80%BB%E7%BB%93/","excerpt":"","text":"XSS，跨站脚本攻击XSS的根本原因是，前端页面被嵌入一些恶意代码，这些恶意代码可能通过不同途径，注入进来。根据不同注入途径（或着说方式），可以分为反射型、持久型。 反射型XSS 恶意伪造url -&gt; 骗取用户点击 -&gt; 页面从url取参数进行渲染。从而参数里的恶意代码被执行。 案例：微博hellosamy事件 持久性XSS 在留言板、评论等场景提交恶意代码 -&gt; 后台未经处理，直接保存了前端提交的数据，-&gt; 再次访问或其他人访问时，前端展示相关内容，又把这些数据取出来进行渲染，从而恶意代码被执行。 案例：微信公众号XSS事件 应对： 用户提交的数据，入库前预处理，很多xssfilter 前端拼接Html时，也要做充分转义 为了防止cookie盗用，重要cookie设置http-only为true 参考 https://tech.meituan.com/2018/09/27/fe-security.html CSRF，跨站请求伪造（英語：Cross-site request forgery）本质是浏览器在发起请求时，会自动带上对应域名下的cookie。该特性可能导致，用户在访问恶意网站时，在用户不知不觉的情况下，触发一些携带了用户身份信息（cookie）的请求。如下图所示： 银行网站A，它以GET请求来完成银行转账的操作，如：http://www.mybank.com/Transfer.php?toBankId=11&amp;money=1000 危险网站B，它里面有一段HTML的代码如下： &lt;img src=http://www.mybank.com/Transfer.php?toBankId=11&amp;money=1000&gt; 首先，你登录了银行网站A，然后访问危险网站B，噢，这时你会发现你的银行账户少了1000块…… 在访问危险网站B的之前，你已经登录了银行网站A，而B中的img以GET的方式请求第三方资源（这里的第三方就是指银行网站了，原本这是一个合法的请求，但这里被不法分子利用了），你的浏览器会带上你的银行网站A的Cookie发出Get请求，去获取src指向的资源，结果银行网站服务器收到请求后，判断身份通过，所以就立刻进行转账操作…… 该例子里，一方面是由于用户上了小网站，另一方面，不应该用GET请求去更新资源（更改账户）。因为像src/script等标签都是默认用GET获取资源，如果再对前端熟悉一些的，可能会想到jsonp，就是利用script标签实现的。很多邮箱图片默认不展示，CSRF也是原因之一。当然还有很多其他的风险，这个可以单开一篇，开开脑洞。 vs 跨域？很多人会有疑问，浏览器不是有跨域限制吗，为什么还会出现在A页面，访问B服务器的情况。对于跨域请求，浏览器还是会正常发出，收到response后，会判断源和当前页面的源是否是属于同源，如果不属于，则需要根据access-control-allow-origin等header，判断server端是否允许跨域。 应对主要通过两个关键点： 虽然A网站可以向B服务器发请求，但是由于跨域限制，没法处理对应的response。因此一些更新资源的操作，最好用POST，更好的是使用restful风格。另一方面，也可以增加二次确认，比如引入验证码，实际上相当于一个动态的token。 由于正规的浏览器，对cookie访问，也要求同源。因此可以再query里增加一些cookie里才有的信息，在服务端校验query和cookie里对应的参数，如果不一致则为恶意。 其他的方法，还有增加referer，但是有的时候请求不带referer，比如非http协议页面发出的请求（ftp之类的）、https页面发出的http请求等，因此该方法有一定的漏洞。 SQL注入关键点： 1. 不要相信请求携带的参数，不要直接拿过来拼接SQL语句。SQL注入的防范很成熟，使用prepare statement即可，常用的client lib里都会实现。但是表名不支持参数化，因此表名还是得使用代码拼接的方式。这就要求表名不能是前端输入的，或者增加表名白名单校验。 从一条sql执行过程来说，编译 -&gt; 执行。一般情况是连带参数，一起编译，就会出现注入情况。 使用参数化查询的形式，会提前对模板进行预编译，而每个?占位的参数，只会被数据库当做一个完整的参数处理。","categories":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/categories/%E5%AE%89%E5%85%A8/"}],"tags":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/tags/%E5%AE%89%E5%85%A8/"}]},{"title":"MySQL知识点","slug":"MySQL知识点","date":"2020-05-14T03:09:35.000Z","updated":"2020-05-14T03:09:35.000Z","comments":true,"path":"2020/05/14/MySQL知识点/","link":"","permalink":"http://yoursite.com/2020/05/14/MySQL%E7%9F%A5%E8%AF%86%E7%82%B9/","excerpt":"","text":"范式与反范式 范式 描述 反例 第一范式 每个字段都是原子的，不能再分解 某个字段是json串 第二范式 1. 表必须有主键；2. 非主属性，必须完全依赖主键，而不能只依赖主键的一部分字段。 好友关系表，关注人ID+被关注人ID作为主键，还存储了关注人的头像，这个只依赖于主键的一个字段。 第三范式 非主属性，直接依赖主键，而非间接依赖。 员工表，有部门ID和部门名称等，部门名称依赖部门ID，而不是员工ID，不应在员工表中。 分库分表比如电商订单表，有三个查询纬度：订单ID，用户ID，商户ID。 建立主纬度和辅助纬度之间的一个映射表比如，以订单ID拆分，那么要保存用户ID-&gt;订单ID和商户ID-&gt;订单ID的映射表。然而问题是： 映射表本身也要分表 每个订单，要写入多个库，属于分布式事务问题。通常会由后台任务，定时对比，保证多库表最终一致。 业务双写存多份数据，但是拆分纬度不一样。一套按用户ID划分，一套按商户号划分。同样存在写入多个库的分布式事务问题。 异步双写还是多份数据，业务单写一份，然后通过监听binlog，同步到其他表上 多个纬度统一到一个纬度比如把订单ID和用户ID统一成一个维度，然后把用户ID作为订单ID的一部分。这样，订单ID中就包含了用户ID的信息，然后按照用户ID分库，当按订单ID查询的时候，提取出用户ID，再按用户ID查询。 总之就是，拆分依据的维度，要同时在多个原始ID中提现 分库分表后的Join问题 join拆分为多个单表查询，在应用层代码里做join处理 增加宽表，提前join好 利用搜索引擎，比如ES，将DB数据导入ES中 分布式事务 最好是优化业务，避免跨库事务 如果无法避免，参考笔记：分布式事务一致性 B+树1. 优点相比hash索引，以及类似结构的KV缓存或数据库，有以下特性 范围查询 前缀匹配，模糊查询 排序和分页 2. 物理结构 磁盘属于块设备，innoDB读写磁盘，是以page为基本单位，page默认大小是16KB，每次I/O都是16KB的整数倍。 innoDB为每个Page赋予一个32位的全局编号，因此innoDB的存储上限是64T (2^32 * 16KB)。如果用来装非叶子节点，假如key是64位整数，也就是8字节，加上其他字段，按16字节算，一个page可以装1000个key。基于此估算，一个三层的B+树，可以存储的数据量： 第一层：根节点，一个page，1000个key。16KB内存，对应1000个子节点 第二层：1000个节点，每个节点一个page，每个page又可以有1000个子节点。16MB内存，对应1000 * 1000个子节点 第三层：1000 * 1000个节点，每个节点一个page。那么该表的最大容量是：1000 * 1000 * 16KB = 16GB。只需要16MB的内存索引，只需要一次I/O读取叶子节点 叶子page内部，以单向链表的方式，存储一条条的记录 非主键索引，索引树叶子节点存的是主键的value。 事务与锁1. 隔离级别 隔离级别 解决问题 Read Uncommited Read commited 解决脏读 Repeatable Read 解决幻读（通过间隙锁），innoDB默认级别。MVCC需要结合行锁，实现当前读，解决update时的覆盖问题。 Serialization 2. 死锁检测 判断一个有向图是否存在环，dfs、拓扑排序 死锁的发生，与代码有关，也与事务隔离级别有关，因为隔离级别会影响加锁机制。 复杂度是O(N) 3. innoDB的MVCC实现 每一行都有两个隐藏列，最近修改的事务ID + undolog里回滚段指针（便于回滚） 一致性视图，{low_trx_id, up_trx_id, trx_ids} low_trx_id: 当前事务链表，最小的事务id up_trx_id: 当前事务链表，最大的事务id trx_ids: 正在执行的事务的id集合通过比较当前事务id，与以上三个变量的关系，确定某个版本数据，是否对当前事务可见。 4. 事务实现1. WAL, Write-Ahead Log内存操作数据 + write-ahead log 2. Redo Log的逻辑与物理结构 redo log 物理组成结构 一个逻辑事务 包含 多个物理事务mtr，Mini Transaction 每个mtr对应一个LSN 一个LSN对应若干个连续的block 这些block，最终组成了 redo log 综上，一个事务在redo log里，可能有多个LSN，这些LSN自己是连续的，但是多个LSN不一定是连续的。 redo log 日志内容格式 先以page为单位记录日志 在每个page里面再采用物理记法 比如 (page id, record offset, (field1, value1)..(fieldi, valuei)…) Aries恢复算法 分析阶段从上一个checkpoint开始，开始分析哪些事务执行完了，未刷写page；哪些事务执行了一半，需要回滚。checkpoint机制，可以加快分析速度 redo阶段对已经commit的事务，执行redolog，刷写page。redolog是幂等的，重复执行没关系。 undo阶段对于未commit的事务，执行undolog，回滚 其他 每个page上记录了，上次修改的LSN，因此恢复时，如果redolog里的lsn&lt;page lsn，说明不用重写了。 redolog保证的是事务的持久性，写入成功，则不会丢失 3. Undo log redolog按LSN的顺序，而undolog没有顺序，多个事务并行写。每条日志除下记录主键ID和数据外，还有两个字段：修改记录的事务ID和回滚指针，用来串联所有历史版本，就是MVCC的两个隐藏列。 undo log 只在commit的过程中有用，一旦事务commit了，就可以删掉undo log 通俗一点，修改行前，先把行拷贝一份出来，这些历史版本形成一个链表。 各种锁 有不同的划分标准，比如按粒度，有表锁、行锁、gap锁；按锁的模式，有共享锁、排他锁、意向锁等 MySQL加锁问题与隔离级别有关，如果隔离级别是Read Commited，则不需要gap锁，因为RC允许幻读。 具体到各种锁 全局锁：对整个DB加锁，一些不支持事务的引擎，可以在备份前，锁住DB MDL，元数据锁：MDL分读/写，不需显式调用。MDL也是在语句执行时隐式加，在事务提交后释放。比如在对表做CURD时，加MDL读锁；对表做DDL时，加MDL写锁。 表锁，读/写，共享/排他，S/X 行锁，读/写，共享/排他，S/X 意向锁，意向锁也是表级别，但是意向锁之间互不排斥，包括IX（意向写）与IX也不互斥。意向锁的目的是提高在加表锁时的判断效率。如果事务要给表中某一行加X锁，首先要对表加IX锁；如果要给某一行加S锁，就先对表加IS锁。这也是“意向”一词的含义。如果一个事务要对表加X锁，就可以根据表有没有被其他事务加IS/IX锁，就可得知，有没有其他事务在读写该表。 间隙锁，解决幻读问题 AI锁，表级别，针对自增ID生成器，如果事务rollback，自增ID一列会不连续 其他问题 double write 机制 InnoDB的page size一般是16KB，其数据校验也是针对这16KB来计算的，将数据写入到磁盘是以page为单位进行操作的。操作系统写文件是以4KB作为单位的，磁盘IO是以512字节为单位的，那么每写一个InnoDB的page到磁盘上，操作系统需要写4个块。而计算机硬件和操作系统，在极端情况下（比如断电）往往并不能保证这一操作的原子性，16K的数据，写入4K时，发生了系统断电或系统崩溃，只有一部分写是成功的，这种情况下就是partial page write（部分页写入）问题。这时page数据出现不一样的情形，从而形成一个”断裂”的page，使数据产生混乱。这个时候InnoDB对这种块错误是无 能为力的. 有人会认为系统恢复后，MySQL可以根据redo log进行恢复，而MySQL在恢复的过程中是检查page的checksum，checksum就是pgae的最后事务号，发生partial page write问题时，page已经损坏，找不到该page中的事务号，就无法恢复。 为了解决该问题，写数据page时，写两遍到磁盘，第一遍是写到double write buffer文件上, 第二遍是从double write buffer写到真正的数据文件中。如果宕机重启，发现page损坏，可以从double write buffer中恢复。 因为redo log的写入单位就是512字节，也就是磁盘IO的最小单位，因此可以保证原子性，不会导致数据损坏。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}]},{"title":"对CAP的正确理解","slug":"CAP","date":"2020-05-14T03:06:27.000Z","updated":"2020-05-14T03:06:27.000Z","comments":true,"path":"2020/05/14/CAP/","link":"","permalink":"http://yoursite.com/2020/05/14/CAP/","excerpt":"","text":"CAP C，一致性，多副本一致性，事务一致性等 A，可用性 P，分区容忍性 理解 最大的误解：CAP可以三选二实际上P是必然存在的，只能在C和A（一致性和可用性）之间权衡。实际中大多是AP或CP系统，很少有CA的系统。 AP系统，追求可用性，放弃一致性。比如MySQL主从等。 CP系统，追求强一致性，牺牲一定的可用性。raft、zab协议。而此时的一致性，也只是对客户端看来是一致的，对内部看，是最终一致，因为同步数据总需要时间。 对于CA系统，因为要实现A（高可用），就必然有冗余，有冗余就必然存在P。比如MySQL，内部事务实现强一致性C，但是没有A，单机也不存在P。 只要引入冗余，实现的高可用（A），就一定存在P。如果还想兼顾一致性（C），那么一定不是真的A。因此实际系统中，总是在CA之间做权衡。放弃某一方，就变成了AP或CP。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"Map并发安全实现原理","slug":"map并发安全实现原理","date":"2020-05-14T03:03:32.000Z","updated":"2020-05-14T03:03:32.000Z","comments":true,"path":"2020/05/14/map并发安全实现原理/","link":"","permalink":"http://yoursite.com/2020/05/14/map%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"Java Concurrent hashmap 多个segment，支持最大segment数量的并发访问 ps: 如果hash桶的list过长，可以使用红黑树代替list golang sync.Map read-only, dirty 两个字段将读写分离 read-only不需加锁，读或写dirty都需要加锁 misses字段，统计read-only穿透次数，超过一定次数将dirty同步到read-only上 删除时，通过给read-only添加标记，延迟删除 读的时候，先查询read，不存在时查询dirty；写入时则只写入dirty 写入过程，每次写入时，先copy 未删除的read-only到dirty中，然后将k-v存入dirty。 read-only可以当做dirty的缓存。dirty里的数据，总比read-only的多。 适用于读多写少的场景。写入较多时，性能无法保证。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"各种树结构","slug":"各种树结构","date":"2020-05-13T02:21:07.000Z","updated":"2020-05-13T02:21:07.000Z","comments":true,"path":"2020/05/13/各种树结构/","link":"","permalink":"http://yoursite.com/2020/05/13/%E5%90%84%E7%A7%8D%E6%A0%91%E7%BB%93%E6%9E%84/","excerpt":"","text":"B树 vs B+树 vs B*树 B树，B是指发明人的名字 平衡多路搜索树 保持键值有序，以顺序遍历 使用不完全填充的节点块，来加速插入和删除 节点块至少半满，提升空间利用率 B+树 VS B树 非叶子节点，只保存索引：从而可以减少索引树的大小，内存里可以保存更多的索引。由于每次都需要走到叶子节点，查询时间也更稳定。 叶子节点之间，增加链指针，方便遍历 B*树在B+树的基础上 非根和非叶子节点，增加指向兄弟的指针 插入时，如果节点已满，会检查兄弟节点是否满，未满，则向兄弟节点转移数据；已满，则从当前节点和兄弟节点，各拿出1/3数据，创建一个新节点。从而节点空间利用率更高，节点分裂的情况也减少。 红黑树 也是一种BST(二叉搜索树)，但是不要求完全平衡 牺牲部分平衡性，达到较快的插入和删除性能 使用场景：linux CFS调度，nginx timer等 vs B树: B树作为多路搜索，能够在树深较小的情况下，支持更多的数据节点。对于磁盘类操作，可以避免大量的随机IO（一个磁盘page，可以读取到更多的索引，类似MySQL），从而优化读写性能。而红黑树一般整棵树都在内存里，不涉及到磁盘操作，支持的数据量较小，但是由于各种操作优于BST，因此常用于涉及到排序、搜索的场景。比如CFS，为了保证公平调度，每次选取当前执行总时间最小的线程执行。 LSM，Log-Structured Merged Tree 核心思想：放弃部分读性能，提高写性能。适用于kv存储 应用：rocksDB，levelDB，hbase rocksDB：c++编写的kv存储引擎 levelDB：kv存储引擎 hbase: 分布式存储，列数据库，应对大量数据（亿级以上） 内存中的memtable，磁盘上的sstable。读取的时候，需要遍历sstable，这里的 优化是，使用是bloom filter，确定一个Key是否在sstable里。 一般LSM-Trees会配合内存排序，内存里将写数据缓冲（通常是一个(Red-Black Tree)红黑树结构）。等积累得足够多之后，使用归并排序将数据合并，写入磁盘。由于。 参考资料 http://blog.fatedier.com/2016/06/15/learn-lsm-tree/ lsm vs b+树 查询过程为了快速查询，一个办法是建立hash索引，但是hash索引占用空间太大，而且不支持区间查询。另一个办法是，事先对数据进行排序，B+树，把排序的操作放在了写入的时候，读的时候便轻松一些。 写过程 但是B树面对高并发写的时候，压力很大。B树把所有的压力都放到了写操作的时候，从根节点索引到数据存储的位置，可能需要多次读文件；真正插入的时候，又可能会引起page的分裂，多次写文件。 LSM在写的时候，直接写入内存，然后利用红黑树保持内存中的数据有序，由后台线程定期或被触发，去merge和持久化到磁盘。也会使用WAL方式记录log，避免数据丢失。 当写比读多时，LSM树相比于B树有更好的性能。因为随着insert操作，为了维护B树结构，节点分裂。读磁盘的随机读写概率会变大，性能会逐渐减弱。LSM把多次IO，批量变成一次IO，复用了磁盘寻道时间，极大提升效率。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"分布式锁","slug":"分布式锁","date":"2020-05-12T13:35:11.000Z","updated":"2020-05-12T13:35:11.000Z","comments":true,"path":"2020/05/12/分布式锁/","link":"","permalink":"http://yoursite.com/2020/05/12/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","excerpt":"","text":"redis set key val nx ex 优点：实现简单，性能好 缺点：超时时间不好控制，极端情况，会出现超时后，多个节点获取到同一把锁的情况。 问题 主从，redis主从采用异步复制，那么如果主机宕机，切换到从，会导致部分锁数据丢失。此时，多个client会拿到同一把锁。 如果锁没有设置超时，若client挂掉，则锁永远不会释放 如果锁设置了超时，若client阻塞或业务执行超时，也会导致多个client拿到同一把锁。 zookeeper 使用临时顺序节点，如果自己是子节点的第一个，则表示加锁成功。否则，watch上一个，如果上一个释放，表示轮到自己了。 优点：一般情况，不存在client宕机/超时问题，zk感知到client宕机，会自动删除对应的临时顺序节点，相当于自动释放锁，或者取消自己的排队。 缺点：实现复杂，吞吐量不高 问题 因为zk使用心跳判断client是否在线，如果网络超时或者full GC等等，导致zk认为client宕机，则会释放锁。导致其他client同时获得该锁。但是这种情况很少见，相比之下，client处理超时这种更常见，这也是zk比redis方案好的原因。 mysql行锁 优点：不需引入额外中间件 缺点：吞吐量不高；也存在client宕机超时问题 总结 探测client是否宕机很难，如果因为超时，那就不应该释放锁。如果是因为宕机，那就应该释放锁。 没有完美的方案，实际场景中，分布式锁只应作为辅助手段，比如为了减少DB的压力等，不应仅靠它控制业务并发逻辑。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"方案总结","slug":"分布式/方案总结","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%96%B9%E6%A1%88%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"高并发常见方案","slug":"高并发","date":"2020-05-12T13:29:38.000Z","updated":"2020-05-12T13:29:38.000Z","comments":true,"path":"2020/05/12/高并发/","link":"","permalink":"http://yoursite.com/2020/05/12/%E9%AB%98%E5%B9%B6%E5%8F%91/","excerpt":"","text":"高并发写 数据分片 数据库分库分表 JDK concurrentHashMap实现 kafka的partition ES的分布式索引 任务分片 CPU的指令流水线 Map/Reduce Tomcat 1+N+M 网络模型：1个监听线程，N个IO线程负责对socket进行读写，M个worker对请求做逻辑处理。 异步化：异步接口、异步IO 短信验证码注册/登录 订单系统 广告计费系统，异步，多消息合并扣费 Kafka的Pipeline WAL技术 数据库redo log LSM树 批量 kafka的百万qps写入:partition分片，磁盘顺序写入，批量（leader/follower之间的批量，本地client之间的批量） mysql的group commit机制，对多事务的redo log批量flush 高并发读 加缓存 本地缓存/redis/memcached 增加副本冗余 MySQL master/slave CDN 静态文件加速 并发读 异步RPC 冗余请求，降低失败率","categories":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"方案总结","slug":"分布式/方案总结","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%96%B9%E6%A1%88%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"多副本一致性","slug":"多副本一致性","date":"2020-05-12T03:23:33.000Z","updated":"2020-05-12T05:09:02.394Z","comments":true,"path":"2020/05/12/多副本一致性/","link":"","permalink":"http://yoursite.com/2020/05/12/%E5%A4%9A%E5%89%AF%E6%9C%AC%E4%B8%80%E8%87%B4%E6%80%A7/","excerpt":"","text":"同步本质每台机器都把收到的请求按日志存下来，各机器的日志文件保持一致。选择存储“事件流”，而非最终状态，原因是： 日志只有一种操作，append，相对简单 Paxos算法1. Basic Paxos 两个角色，Proposer 和 Acceptor，以及一个自增ID（n） 两个阶段，Propose阶段 和 Accept 阶段 Propose阶段 proposer广播消息，id为n，prepare(n) acceptor接收消息，如果n &gt; local N，则回复yes proposer收到半数以上的yes，开始广播，否则id自增，重新广播 Acctpt阶段 proposer广播消息, accept(n, value) acceptor接收消息，如果n &gt; loacal N，则持久化，返回yes proposer收到半数以上的yes，则结束。否则id自增，从proposer阶段重新开始。 两个问题 Paxos是个不断循环的2PC，有可能陷入死循环，所谓“活锁”。比如3个node同时propose，都收到no，又同时n++，继续propose，继续no 性能：每次写入，需要两次RTT + 两次写盘。两次RTT分别是Propose/Accept阶段。这两个阶段都会持久化一些变量，需要磁盘IO。 活锁问题 多点写入，变为单点写入。选出一个leader，只让leader当proposer。从而减少冲突。leader选取办法，比如每个节点增加编号，使用心跳，选取编号最大的节点为leader。即使出现同一时间，多个leader，也不影响paxos的正确性，只会增大并发写冲突的概率。 Raft算法 单点写入：任一时刻，只允许一个有效的leader存在，所有的写请求，都传到leader上，然后由leader同步给超过半数的follower。 单条日志结构：term + index + content。term是leader的任期，只会单调递增；index是日志顺序编号，也是递增； 分为三个阶段，选举阶段，正常阶段，恢复阶段 选举阶段 节点有三个状态：leader、follower、candidate。candidate是个中间状态。 当follower在一定时间收不到leader心跳时，就会随机sleep一个时间，然后变为candidate，发起选举。选举结束后，变为leader或follower。 选举算法，保证同一时间只有一个leader。 如果选举请求里，日志的term和index比自己本地的新，则返回true，否则返回false。 candidate收到多数派返回true，则成为leader 每个节点只能投一次true，防止多个leader。因此选取出的leader不一定是最新的，但一定比大多数节点新。 正常阶段，复制日志 只要超过半数的follower复制成功，就返回给客户端日志写入成功。 关键的日志一致性保证： 如果两个节点的日志，index和term相同，则内容一定相同。 如果index=M处的日志相同，则在M之前的日志，也一定相同。 恢复阶段 leader同步term给follower 以leader本地的日志为基准，复制给follower 安全性保证 leader数据是基准，leader不会从别的节点同步数据，只会是别的节点根据leader数据删除或追加自己的数据。 对于已经commit的日志，一定是commit的。对于新任leader上，前任leader未commit的日志，稍后会变为commit状态。不在新任leader上的未commit数据，会被覆盖。 Zabzookeeper使用的强一致性算法，同时也是单点写入，写请求都转发给leader。 模型对比，复制状态机(replicated state machine, paxos/raft) vs 主备系统（primay-backup system，zab）,前者持久化的是客户端的请求序列（日志序列），另外一个持久化的是数据的状态变化。 数据同步次数不一样，如果client执行三次x=1，后两次在主备系统里，不用触发同步。 存储状态变化，具有幂等性，而复制状态机不具备。 zxid 高32位，leader任期，类似raft的term 低32位，日志序列，类似raft的日志index 三个阶段：Leader选举，BroadCast,恢复阶段 Leader选举：FLE算法 Leader和Follower之间是双向心跳；raft里是单向 选取zxid最大的节点作为leader；和raft选取term+index最新的节点作为leader一个意思。 broadcast阶段 raft vs zab参考：https://my.oschina.net/pingpangkuangmo/blog/782702 上一轮残留的数据怎么处理？ 首先看下上一轮次的leader在挂或者失去leader位置之前，会有哪些数据？ 已过半复制的日志 未过半复制的日志一个日志是否被过半复制，是否被提交，这些信息是由leader才能知晓的， 那么下一个leader该如何来判定这些日志呢？ 下面分别来看看Raft和ZooKeeper的处理策略： Raft：对于之前term的过半或未过半复制的日志采取的是保守的策略，全部判定为未提交，只有当当前term的日志过半了，才会顺便将之前term的日志进行提交 ZooKeeper：采取激进的策略，对于所有过半还是未过半的日志都判定为提交，都将其应用到状态机中 Raft的保守策略更多是因为Raft在leader选举完成之后，没有同步更新过程来保持和leader一致（在可以对外服务之前的这一同步过程）。而ZooKeeper是有该过程的","categories":[{"name":"后端","slug":"后端","permalink":"http://yoursite.com/categories/%E5%90%8E%E7%AB%AF/"},{"name":"系统原理","slug":"后端/系统原理","permalink":"http://yoursite.com/categories/%E5%90%8E%E7%AB%AF/%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"开天辟地","slug":"开天辟地","date":"2020-05-11T09:05:19.000Z","updated":"2020-05-14T03:04:18.669Z","comments":true,"path":"2020/05/11/开天辟地/","link":"","permalink":"http://yoursite.com/2020/05/11/%E5%BC%80%E5%A4%A9%E8%BE%9F%E5%9C%B0/","excerpt":"","text":"欢迎","categories":[],"tags":[]}],"categories":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/categories/%E5%AE%89%E5%85%A8/"},{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"},{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"方案总结","slug":"分布式/方案总结","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%96%B9%E6%A1%88%E6%80%BB%E7%BB%93/"},{"name":"后端","slug":"后端","permalink":"http://yoursite.com/categories/%E5%90%8E%E7%AB%AF/"},{"name":"系统原理","slug":"后端/系统原理","permalink":"http://yoursite.com/categories/%E5%90%8E%E7%AB%AF/%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86/"}],"tags":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/tags/%E5%AE%89%E5%85%A8/"},{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"},{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]}