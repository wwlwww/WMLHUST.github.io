<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小本本</title>
  
  <subtitle>学而时习之，不亦说乎</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-05-14T08:20:55.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>WordGe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>常见web安全总结</title>
    <link href="http://yoursite.com/2020/05/14/%E5%B8%B8%E8%A7%81web%E5%AE%89%E5%85%A8%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2020/05/14/%E5%B8%B8%E8%A7%81web%E5%AE%89%E5%85%A8%E6%80%BB%E7%BB%93/</id>
    <published>2020-05-14T08:20:55.000Z</published>
    <updated>2020-05-14T08:20:55.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="XSS，跨站脚本攻击"><a href="#XSS，跨站脚本攻击" class="headerlink" title="XSS，跨站脚本攻击"></a>XSS，跨站脚本攻击</h3><p>XSS的根本原因是，前端页面被嵌入一些恶意代码，这些恶意代码可能通过不同途径，注入进来。根据不同注入途径（或着说方式），可以分为反射型、持久型。</p><ol><li>反射型XSS   </li></ol><ul><li>恶意伪造url -&gt; 骗取用户点击 -&gt; 页面从url取参数进行渲染。从而参数里的恶意代码被执行。</li><li>案例：微博hellosamy事件</li></ul><ol start="2"><li>持久性XSS   </li></ol><ul><li>在留言板、评论等场景提交恶意代码 -&gt; 后台未经处理，直接保存了前端提交的数据，-&gt; 再次访问或其他人访问时，前端展示相关内容，又把这些数据取出来进行渲染，从而恶意代码被执行。  </li><li>案例：微信公众号XSS事件</li></ul><ol start="3"><li><p>应对：</p><ol><li>用户提交的数据，入库前预处理，很多xssfilter</li><li>前端拼接Html时，也要做充分转义</li><li>为了防止cookie盗用，重要cookie设置http-only为true</li></ol></li><li><p>参考   </p><blockquote><p><code>https://tech.meituan.com/2018/09/27/fe-security.html</code></p></blockquote></li></ol><h3 id="CSRF，跨站请求伪造（英語：Cross-site-request-forgery）"><a href="#CSRF，跨站请求伪造（英語：Cross-site-request-forgery）" class="headerlink" title="CSRF，跨站请求伪造（英語：Cross-site request forgery）"></a>CSRF，跨站请求伪造（英語：Cross-site request forgery）</h3><p>本质是浏览器在发起请求时，会自动带上对应域名下的cookie。该特性可能导致，用户在访问恶意网站时，在用户不知不觉的情况下，触发一些携带了用户身份信息（cookie）的请求。如下图所示：<br><img src="/images/csrf.jpg" alt="CSRF示意图"></p><blockquote><ol><li>银行网站A，它以GET请求来完成银行转账的操作，如：<a href="http://www.mybank.com/Transfer.php?toBankId=11&amp;money=1000" target="_blank" rel="noopener">http://www.mybank.com/Transfer.php?toBankId=11&amp;money=1000</a></li><li>危险网站B，它里面有一段HTML的代码如下：<pre><code class="html">　　<span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">http://www.mybank.com/Transfer.php?toBankId</span>=<span class="string">11&amp;money</span>=<span class="string">1000</span>&gt;</span></code></pre></li><li>首先，你登录了银行网站A，然后访问危险网站B，噢，这时你会发现你的银行账户少了1000块……</li></ol></blockquote><p>在访问危险网站B的之前，你已经登录了银行网站A，而B中的img以GET的方式请求第三方资源（这里的第三方就是指银行网站了，原本这是一个合法的请求，但这里被不法分子利用了），你的浏览器会带上你的银行网站A的Cookie发出Get请求，去获取src指向的资源，结果银行网站服务器收到请求后，判断身份通过，所以就立刻进行转账操作……</p><p>该例子里，一方面是由于用户上了小网站，另一方面，不应该用GET请求去更新资源（更改账户）。因为像src/script等标签都是默认用GET获取资源，如果再对前端熟悉一些的，可能会想到jsonp，就是利用script标签实现的。<br>很多邮箱图片默认不展示，CSRF也是原因之一。当然还有很多其他的风险，这个可以单开一篇，开开脑洞。</p><ol><li><p>vs 跨域？<br>很多人会有疑问，浏览器不是有跨域限制吗，为什么还会出现在A页面，访问B服务器的情况。对于跨域请求，<strong>浏览器还是会正常发出</strong>，收到response后，会判断源和当前页面的源是否是属于同源，如果不属于，则需要根据<code>access-control-allow-origin</code>等header，判断server端是否允许跨域。   </p></li><li><p>应对<br>主要通过两个关键点：   </p><ol><li>虽然A网站可以向B服务器发请求，但是由于跨域限制，没法处理对应的response。因此一些更新资源的操作，最好用POST，更好的是使用restful风格。另一方面，也可以增加二次确认，比如引入验证码，实际上相当于一个动态的token。</li><li>由于正规的浏览器，对cookie访问，也要求同源。因此可以再query里增加一些cookie里才有的信息，在服务端校验query和cookie里对应的参数，如果不一致则为恶意。   </li></ol></li></ol><p>其他的方法，还有增加referer，但是有的时候请求不带referer，比如非http协议页面发出的请求（ftp之类的）、https页面发出的http请求等，因此该方法有一定的漏洞。</p><h3 id="SQL注入"><a href="#SQL注入" class="headerlink" title="SQL注入"></a>SQL注入</h3><p>关键点：<br>    1. 不要相信请求携带的参数，不要直接拿过来拼接SQL语句。<br>SQL注入的防范很成熟，使用prepare statement即可，常用的client lib里都会实现。但是表名不支持参数化，因此表名还是得使用代码拼接的方式。这就要求表名不能是前端输入的，或者增加表名白名单校验。   </p><blockquote><p>从一条sql执行过程来说，编译 -&gt; 执行。一般情况是连带参数，一起编译，就会出现注入情况。<br>  使用参数化查询的形式，会提前对模板进行预编译，而每个?占位的参数，只会被数据库当做一个完整的参数处理。</p></blockquote>]]></content>
    
    <summary type="html">
    
      总结常见的几种Web安全问题，包括XSS、CSRF、SQL注入。
    
    </summary>
    
    
      <category term="安全" scheme="http://yoursite.com/categories/%E5%AE%89%E5%85%A8/"/>
    
    
      <category term="安全" scheme="http://yoursite.com/tags/%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>MySQL知识点</title>
    <link href="http://yoursite.com/2020/05/14/MySQL%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    <id>http://yoursite.com/2020/05/14/MySQL%E7%9F%A5%E8%AF%86%E7%82%B9/</id>
    <published>2020-05-14T03:09:35.000Z</published>
    <updated>2020-05-14T03:09:35.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="范式与反范式"><a href="#范式与反范式" class="headerlink" title="范式与反范式"></a>范式与反范式</h2><table><thead><tr><th>范式</th><th>描述</th><th>反例</th></tr></thead><tbody><tr><td>第一范式</td><td>每个字段都是原子的，不能再分解</td><td>某个字段是json串</td></tr><tr><td>第二范式</td><td>1. 表必须有主键；2. 非主属性，必须完全依赖主键，而不能只依赖主键的一部分字段。</td><td>好友关系表，关注人ID+被关注人ID作为主键，还存储了关注人的头像，这个只依赖于主键的一个字段。</td></tr><tr><td>第三范式</td><td>非主属性，直接依赖主键，而非间接依赖。</td><td>员工表，有部门ID和部门名称等，部门名称依赖部门ID，而不是员工ID，不应在员工表中。</td></tr></tbody></table><h2 id="分库分表"><a href="#分库分表" class="headerlink" title="分库分表"></a>分库分表</h2><p>比如电商订单表，有三个查询纬度：订单ID，用户ID，商户ID。</p><ol><li>建立主纬度和辅助纬度之间的一个映射表<br>比如，以订单ID拆分，那么要保存用户ID-&gt;订单ID和商户ID-&gt;订单ID的映射表。然而问题是：<ol><li>映射表本身也要分表</li><li>每个订单，要写入多个库，属于分布式事务问题。通常会由后台任务，定时对比，保证多库表最终一致。</li></ol></li><li>业务双写<br>存多份数据，但是拆分纬度不一样。一套按用户ID划分，一套按商户号划分。同样存在写入多个库的分布式事务问题。</li><li>异步双写<br>还是多份数据，业务单写一份，然后通过监听binlog，同步到其他表上</li><li>多个纬度统一到一个纬度<br>比如把订单ID和用户ID统一成一个维度，然后把用户ID作为订单ID的一部分。这样，订单ID中就包含了用户ID的信息，然后按照用户ID分库，当按订单ID查询的时候，提取出用户ID，再按用户ID查询。   </li></ol><p><strong>总之就是，拆分依据的维度，要同时在多个原始ID中提现</strong></p><h2 id="分库分表后的Join问题"><a href="#分库分表后的Join问题" class="headerlink" title="分库分表后的Join问题"></a>分库分表后的Join问题</h2><ol><li>join拆分为多个单表查询，在应用层代码里做join处理</li><li>增加宽表，提前join好</li><li>利用搜索引擎，比如ES，将DB数据导入ES中</li></ol><h2 id="分布式事务"><a href="#分布式事务" class="headerlink" title="分布式事务"></a>分布式事务</h2><ol><li>最好是优化业务，避免跨库事务</li><li>如果无法避免，参考笔记：分布式事务一致性</li></ol><h2 id="B-树"><a href="#B-树" class="headerlink" title="B+树"></a>B+树</h2><h3 id="1-优点"><a href="#1-优点" class="headerlink" title="1. 优点"></a>1. 优点</h3><p>相比hash索引，以及类似结构的KV缓存或数据库，有以下特性</p><ol><li>范围查询</li><li>前缀匹配，模糊查询</li><li>排序和分页</li></ol><h3 id="2-物理结构"><a href="#2-物理结构" class="headerlink" title="2. 物理结构"></a>2. 物理结构</h3><ol><li>磁盘属于块设备，innoDB读写磁盘，是以page为基本单位，page默认大小是16KB，每次I/O都是16KB的整数倍。   </li><li>innoDB为每个Page赋予一个32位的全局编号，因此innoDB的存储上限是<strong>64T</strong> (2^32 * 16KB)。<br>如果用来装非叶子节点，假如key是64位整数，也就是8字节，加上其他字段，按16字节算，一个page可以装1000个key。基于此估算，一个三层的B+树，可以存储的数据量：<ol><li>第一层：根节点，一个page，1000个key。16KB内存，对应1000个子节点</li><li>第二层：1000个节点，每个节点一个page，每个page又可以有1000个子节点。16MB内存，对应1000 * 1000个子节点</li><li>第三层：1000 * 1000个节点，每个节点一个page。那么该表的最大容量是：1000 * 1000 * 16KB = 16GB。<strong>只需要16MB的内存索引，只需要一次I/O读取叶子节点</strong>    </li></ol></li><li>叶子page内部，以单向链表的方式，存储一条条的记录</li><li>非主键索引，索引树叶子节点存的是主键的value。</li></ol><h2 id="事务与锁"><a href="#事务与锁" class="headerlink" title="事务与锁"></a>事务与锁</h2><h3 id="1-隔离级别"><a href="#1-隔离级别" class="headerlink" title="1. 隔离级别"></a>1. 隔离级别</h3><table><thead><tr><th>隔离级别</th><th>解决问题</th></tr></thead><tbody><tr><td>Read Uncommited</td><td></td></tr><tr><td>Read commited</td><td>解决脏读</td></tr><tr><td>Repeatable Read</td><td>解决幻读（通过间隙锁），innoDB默认级别。MVCC需要结合行锁，实现当前读，解决update时的覆盖问题。</td></tr><tr><td>Serialization</td><td></td></tr></tbody></table><h3 id="2-死锁检测"><a href="#2-死锁检测" class="headerlink" title="2. 死锁检测"></a>2. 死锁检测</h3><ol><li>判断一个有向图是否存在环，dfs、拓扑排序</li><li>死锁的发生，与代码有关，也与事务隔离级别有关，因为隔离级别会影响加锁机制。</li><li>复杂度是O(N)</li></ol><h3 id="3-innoDB的MVCC实现"><a href="#3-innoDB的MVCC实现" class="headerlink" title="3. innoDB的MVCC实现"></a>3. innoDB的MVCC实现</h3><ol><li>每一行都有两个隐藏列，<strong>最近修改的事务ID</strong> + <strong>undolog里回滚段指针（便于回滚）</strong></li><li>一致性视图，{low_trx_id, up_trx_id, trx_ids}   <ul><li>low_trx_id: 当前事务链表，最小的事务id</li><li>up_trx_id:  当前事务链表，最大的事务id</li><li>trx_ids: 正在执行的事务的id集合<br>通过比较当前事务id，与以上三个变量的关系，确定某个版本数据，是否对当前事务可见。</li></ul></li></ol><h3 id="4-事务实现"><a href="#4-事务实现" class="headerlink" title="4. 事务实现"></a>4. 事务实现</h3><h4 id="1-WAL-Write-Ahead-Log"><a href="#1-WAL-Write-Ahead-Log" class="headerlink" title="1. WAL, Write-Ahead Log"></a>1. WAL, Write-Ahead Log</h4><p>内存操作数据 + write-ahead log</p><h4 id="2-Redo-Log的逻辑与物理结构"><a href="#2-Redo-Log的逻辑与物理结构" class="headerlink" title="2. Redo Log的逻辑与物理结构"></a>2. Redo Log的逻辑与物理结构</h4><ol><li>redo log 物理组成结构<ol><li>一个逻辑事务 包含 多个物理事务mtr，Mini Transaction</li><li>每个mtr对应一个LSN</li><li>一个LSN对应若干个连续的block</li><li>这些block，最终组成了 redo log   <blockquote><p>综上，一个事务在redo log里，可能有多个LSN，这些LSN自己是连续的，但是多个LSN不一定是连续的。</p></blockquote></li></ol></li><li>redo log 日志内容格式<ol><li>先以page为单位记录日志</li><li>在每个page里面再采用物理记法<blockquote><p>比如 (page id, record offset, (field1, value1)..(fieldi, valuei)…)   </p></blockquote></li></ol></li><li>Aries恢复算法<ol><li>分析阶段<br>从上一个checkpoint开始，开始分析哪些事务执行完了，未刷写page；哪些事务执行了一半，需要回滚。checkpoint机制，可以加快分析速度</li><li>redo阶段<br>对已经commit的事务，执行redolog，刷写page。redolog是幂等的，重复执行没关系。</li><li>undo阶段<br>对于未commit的事务，执行undolog，回滚</li></ol></li><li>其他<ol><li>每个page上记录了，上次修改的LSN，因此恢复时，如果redolog里的lsn&lt;page lsn，说明不用重写了。</li><li>redolog保证的是事务的持久性，写入成功，则不会丢失</li></ol></li></ol><h4 id="3-Undo-log"><a href="#3-Undo-log" class="headerlink" title="3. Undo log"></a>3. Undo log</h4><ol><li>redolog按LSN的顺序，而undolog没有顺序，多个事务并行写。每条日志除下记录主键ID和数据外，还有两个字段：修改记录的事务ID和回滚指针，用来串联所有历史版本，就是MVCC的两个隐藏列。</li><li>undo log 只在commit的过程中有用，一旦事务commit了，就可以删掉undo log</li><li>通俗一点，修改行前，先把行拷贝一份出来，这些历史版本形成一个链表。</li></ol><h2 id="各种锁"><a href="#各种锁" class="headerlink" title="各种锁"></a>各种锁</h2><ol><li>有不同的划分标准，比如按粒度，有表锁、行锁、gap锁；按锁的模式，有共享锁、排他锁、意向锁等</li><li>MySQL加锁问题与隔离级别有关，如果隔离级别是Read Commited，则不需要gap锁，因为RC允许幻读。</li><li>具体到各种锁<ol><li><code>全局锁</code>：对整个DB加锁，一些不支持事务的引擎，可以在备份前，锁住DB</li><li><code>MDL</code>，元数据锁：MDL分读/写，不需显式调用。MDL也是在语句执行时隐式加，在事务提交后释放。比如在对表做CURD时，加MDL读锁；对表做DDL时，加MDL写锁。</li><li><code>表锁</code>，读/写，共享/排他，S/X</li><li><code>行锁</code>，读/写，共享/排他，S/X</li><li><code>意向锁</code>，意向锁也是表级别，但是意向锁之间互不排斥，包括IX（意向写）与IX也不互斥。意向锁的目的是提高在加表锁时的判断效率。如果事务要给表中某一行加X锁，首先要对表加IX锁；如果要给某一行加S锁，就先对表加IS锁。这也是“意向”一词的含义。<br>如果一个事务要对表加X锁，就可以根据表有没有被其他事务加IS/IX锁，就可得知，有没有其他事务在读写该表。</li><li><code>间隙锁</code>，解决幻读问题</li><li><code>AI锁</code>，表级别，针对自增ID生成器，如果事务rollback，自增ID一列会不连续</li></ol></li></ol><h2 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h2><ol><li><p>double write 机制   </p><p> InnoDB的page size一般是16KB，其数据校验也是针对这16KB来计算的，将数据写入到磁盘是以page为单位进行操作的。操作系统写文件是以4KB作为单位的，磁盘IO是以512字节为单位的，那么每写一个InnoDB的page到磁盘上，操作系统需要写4个块。而计算机硬件和操作系统，在极端情况下（比如断电）往往并不能保证这一操作的原子性，16K的数据，写入4K时，发生了系统断电或系统崩溃，只有一部分写是成功的，这种情况下就是partial page write（部分页写入）问题。这时page数据出现不一样的情形，从而形成一个”断裂”的page，使数据产生混乱。这个时候InnoDB对这种块错误是无 能为力的.   </p><p> 有人会认为系统恢复后，MySQL可以根据redo log进行恢复，而MySQL在恢复的过程中是检查page的checksum，checksum就是pgae的最后事务号，发生partial page write问题时，page已经损坏，找不到该page中的事务号，就无法恢复。   </p><p> 为了解决该问题，写数据page时，写两遍到磁盘，第一遍是写到double write buffer文件上, 第二遍是从double write buffer写到真正的数据文件中。如果宕机重启，发现page损坏，可以从double write buffer中恢复。   </p><p> 因为redo log的写入单位就是512字节，也就是磁盘IO的最小单位，因此可以保证原子性，不会导致数据损坏。</p></li></ol>]]></content>
    
    <summary type="html">
    
      总结一些mysql的知识点，包括范式、索引、事务、锁等等。
    
    </summary>
    
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="MySQL" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"/>
    
    
      <category term="MySQL" scheme="http://yoursite.com/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>对CAP的正确理解</title>
    <link href="http://yoursite.com/2020/05/14/CAP/"/>
    <id>http://yoursite.com/2020/05/14/CAP/</id>
    <published>2020-05-14T03:06:27.000Z</published>
    <updated>2020-05-14T03:06:27.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="CAP"><a href="#CAP" class="headerlink" title="CAP"></a>CAP</h3><ol><li>C，一致性，多副本一致性，事务一致性等</li><li>A，可用性</li><li>P，分区容忍性</li></ol><h3 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h3><ol><li>最大的误解：<strong>CAP可以三选二</strong><br>实际上P是必然存在的，只能在C和A（一致性和可用性）之间权衡。实际中大多是AP或CP系统，很少有CA的系统。</li><li>AP系统，追求可用性，放弃一致性。比如MySQL主从等。</li><li>CP系统，追求强一致性，牺牲一定的可用性。raft、zab协议。而此时的一致性，也只是对客户端看来是一致的，对内部看，是最终一致，因为同步数据总需要时间。</li><li>对于CA系统，因为要实现A（高可用），就必然有冗余，有冗余就必然存在P。比如MySQL，内部事务实现强一致性C，但是没有A，单机也不存在P。</li><li>只要引入冗余，实现的高可用（A），就一定存在P。如果还想兼顾一致性（C），那么一定不是真的A。<strong>因此实际系统中，总是在CA之间做权衡。放弃某一方，就变成了AP或CP。</strong></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;CAP&quot;&gt;&lt;a href=&quot;#CAP&quot; class=&quot;headerlink&quot; title=&quot;CAP&quot;&gt;&lt;/a&gt;CAP&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;C，一致性，多副本一致性，事务一致性等&lt;/li&gt;
&lt;li&gt;A，可用性&lt;/li&gt;
&lt;li&gt;P，分区容忍性&lt;/li&gt;
&lt;/o
      
    
    </summary>
    
    
      <category term="分布式" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
      <category term="分布式" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Map并发安全实现原理</title>
    <link href="http://yoursite.com/2020/05/14/map%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2020/05/14/map%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/</id>
    <published>2020-05-14T03:03:32.000Z</published>
    <updated>2020-05-14T03:03:32.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Java-Concurrent-hashmap"><a href="#Java-Concurrent-hashmap" class="headerlink" title="Java Concurrent hashmap"></a>Java Concurrent hashmap</h3><ol><li>多个segment，支持最大segment数量的并发访问<blockquote><p>ps: 如果hash桶的list过长，可以使用红黑树代替list</p></blockquote></li></ol><h3 id="golang-sync-Map"><a href="#golang-sync-Map" class="headerlink" title="golang sync.Map"></a>golang sync.Map</h3><ol><li>read-only, dirty 两个字段将读写分离</li><li>read-only不需加锁，读或写dirty都需要加锁</li><li>misses字段，统计read-only穿透次数，超过一定次数将dirty同步到read-only上</li><li>删除时，通过给read-only添加标记，延迟删除</li><li>读的时候，先查询read，不存在时查询dirty；写入时则只写入dirty</li><li>写入过程，每次写入时，先copy 未删除的read-only到dirty中，然后将k-v存入dirty。<blockquote><p>read-only可以当做dirty的缓存。dirty里的数据，总比read-only的多。</p></blockquote></li><li><strong>适用于读多写少的场景。写入较多时，性能无法保证。</strong></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Java-Concurrent-hashmap&quot;&gt;&lt;a href=&quot;#Java-Concurrent-hashmap&quot; class=&quot;headerlink&quot; title=&quot;Java Concurrent hashmap&quot;&gt;&lt;/a&gt;Java Concurrent h
      
    
    </summary>
    
    
      <category term="数据结构" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>各种树结构</title>
    <link href="http://yoursite.com/2020/05/13/%E5%90%84%E7%A7%8D%E6%A0%91%E7%BB%93%E6%9E%84/"/>
    <id>http://yoursite.com/2020/05/13/%E5%90%84%E7%A7%8D%E6%A0%91%E7%BB%93%E6%9E%84/</id>
    <published>2020-05-13T02:21:07.000Z</published>
    <updated>2020-05-13T02:21:07.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="B树-vs-B-树-vs-B-树"><a href="#B树-vs-B-树-vs-B-树" class="headerlink" title="B树 vs B+树 vs B*树"></a>B树 vs B+树 vs B*树</h3><ol><li><p>B树，B是指发明人的名字</p><ul><li>平衡多路搜索树</li><li>保持键值有序，以顺序遍历</li><li>使用不完全填充的节点块，来加速插入和删除</li><li>节点块至少半满，提升空间利用率</li></ul></li><li><p>B+树 VS B树</p><ul><li>非叶子节点，只保存索引：从而可以减少索引树的大小，内存里可以保存更多的索引。由于每次都需要走到叶子节点，查询时间也更稳定。</li><li>叶子节点之间，增加链指针，方便遍历</li></ul></li><li><p>B*树<br>在B+树的基础上</p><ul><li>非根和非叶子节点，增加指向兄弟的指针</li><li>插入时，如果节点已满，会检查兄弟节点是否满，未满，则向兄弟节点转移数据；已满，则从当前节点和兄弟节点，各拿出1/3数据，创建一个新节点。<br>从而节点空间利用率更高，节点分裂的情况也减少。</li></ul></li></ol><h3 id="红黑树"><a href="#红黑树" class="headerlink" title="红黑树"></a>红黑树</h3><ol><li>也是一种BST(二叉搜索树)，但是不要求完全平衡</li><li>牺牲部分平衡性，达到较快的插入和删除性能</li><li>使用场景：linux CFS调度，nginx timer等</li><li>vs B树: B树作为多路搜索，能够在树深较小的情况下，支持更多的数据节点。对于磁盘类操作，可以避免大量的随机IO（一个磁盘page，可以读取到更多的索引，类似MySQL），从而优化读写性能。而红黑树一般整棵树都在内存里，不涉及到磁盘操作，支持的数据量较小，但是由于各种操作优于BST，因此常用于涉及到排序、搜索的场景。比如CFS，为了保证公平调度，每次选取当前执行总时间最小的线程执行。</li></ol><h3 id="LSM，Log-Structured-Merged-Tree"><a href="#LSM，Log-Structured-Merged-Tree" class="headerlink" title="LSM，Log-Structured Merged Tree"></a>LSM，Log-Structured Merged Tree</h3><ol><li>核心思想：<strong>放弃部分读性能，提高写性能。</strong>适用于kv存储</li><li>应用：rocksDB，levelDB，hbase<ul><li>rocksDB：c++编写的kv存储引擎</li><li>levelDB：kv存储引擎</li><li>hbase: 分布式存储，列数据库，应对大量数据（亿级以上）</li></ul></li><li>内存中的memtable，磁盘上的sstable。读取的时候，需要遍历sstable，这里的 优化是，使用是bloom filter，确定一个Key是否在sstable里。</li><li>一般LSM-Trees会配合内存排序，内存里将写数据缓冲（通常是一个(Red-Black Tree)红黑树结构）。等积累得足够多之后，使用归并排序将数据合并，写入磁盘。由于。</li><li>参考资料<ul><li><a href="http://blog.fatedier.com/2016/06/15/learn-lsm-tree/" target="_blank" rel="noopener">http://blog.fatedier.com/2016/06/15/learn-lsm-tree/</a></li></ul></li></ol><h3 id="lsm-vs-b-树"><a href="#lsm-vs-b-树" class="headerlink" title="lsm vs b+树"></a>lsm vs b+树</h3><ol><li><p>查询过程<br>为了快速查询，一个办法是建立hash索引，但是hash索引占用空间太大，而且不支持区间查询。另一个办法是，事先对数据进行排序，B+树，把排序的操作放在了写入的时候，读的时候便轻松一些。   </p></li><li><p>写过程<br> 但是B树面对高并发写的时候，压力很大。B树把所有的压力都放到了写操作的时候，从根节点索引到数据存储的位置，可能需要多次读文件；真正插入的时候，又可能会引起page的分裂，多次写文件。   </p><p> LSM在写的时候，直接写入内存，然后利用红黑树保持内存中的数据有序，由后台线程定期或被触发，去merge和持久化到磁盘。也会使用WAL方式记录log，避免数据丢失。  </p><p> 当写比读多时，LSM树相比于B树有更好的性能。因为随着insert操作，为了维护B树结构，节点分裂。读磁盘的随机读写概率会变大，性能会逐渐减弱。LSM把多次IO，批量变成一次IO，复用了磁盘寻道时间，极大提升效率。</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;B树-vs-B-树-vs-B-树&quot;&gt;&lt;a href=&quot;#B树-vs-B-树-vs-B-树&quot; class=&quot;headerlink&quot; title=&quot;B树 vs B+树 vs B*树&quot;&gt;&lt;/a&gt;B树 vs B+树 vs B*树&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;B树，B是
      
    
    </summary>
    
    
      <category term="数据结构" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>分布式锁</title>
    <link href="http://yoursite.com/2020/05/12/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"/>
    <id>http://yoursite.com/2020/05/12/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</id>
    <published>2020-05-12T13:35:11.000Z</published>
    <updated>2020-05-12T13:35:11.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="redis"><a href="#redis" class="headerlink" title="redis"></a>redis</h3><ol><li>set key val nx ex</li></ol><ul><li>优点：实现简单，性能好</li><li>缺点：超时时间不好控制，极端情况，会出现超时后，多个节点获取到同一把锁的情况。</li></ul><ol start="2"><li>问题<ol><li>主从，redis主从采用异步复制，那么如果主机宕机，切换到从，会导致部分锁数据丢失。此时，多个client会拿到同一把锁。</li><li>如果锁没有设置超时，若client挂掉，则锁永远不会释放</li><li>如果锁设置了超时，若client阻塞或业务执行超时，也会导致多个client拿到同一把锁。</li></ol></li></ol><h3 id="zookeeper"><a href="#zookeeper" class="headerlink" title="zookeeper"></a>zookeeper</h3><ol><li>使用临时顺序节点，如果自己是子节点的第一个，则表示加锁成功。否则，watch上一个，如果上一个释放，表示轮到自己了。</li></ol><ul><li>优点：一般情况，不存在client宕机/超时问题，zk感知到client宕机，会自动删除对应的临时顺序节点，相当于自动释放锁，或者取消自己的排队。</li><li>缺点：实现复杂，吞吐量不高</li></ul><ol start="2"><li>问题<ol><li>因为zk使用心跳判断client是否在线，如果网络超时或者full GC等等，导致zk认为client宕机，则会释放锁。导致其他client同时获得该锁。<strong>但是这种情况很少见，相比之下，client处理超时这种更常见，这也是zk比redis方案好的原因。</strong></li></ol></li></ol><h3 id="mysql行锁"><a href="#mysql行锁" class="headerlink" title="mysql行锁"></a>mysql行锁</h3><ul><li>优点：不需引入额外中间件</li><li>缺点：吞吐量不高；也存在client宕机超时问题</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>探测client是否宕机很难，如果因为超时，那就不应该释放锁。如果是因为宕机，那就应该释放锁。</li><li><code>没有完美的方案，实际场景中，分布式锁只应作为辅助手段，比如为了减少DB的压力等，不应仅靠它控制业务并发逻辑。</code></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;redis&quot;&gt;&lt;a href=&quot;#redis&quot; class=&quot;headerlink&quot; title=&quot;redis&quot;&gt;&lt;/a&gt;redis&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;set key val nx ex&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;优点：实现简单，性能好&lt;/l
      
    
    </summary>
    
    
      <category term="分布式" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="方案总结" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%96%B9%E6%A1%88%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="分布式" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>高并发常见方案</title>
    <link href="http://yoursite.com/2020/05/12/%E9%AB%98%E5%B9%B6%E5%8F%91/"/>
    <id>http://yoursite.com/2020/05/12/%E9%AB%98%E5%B9%B6%E5%8F%91/</id>
    <published>2020-05-12T13:29:38.000Z</published>
    <updated>2020-05-12T13:29:38.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="高并发写"><a href="#高并发写" class="headerlink" title="高并发写"></a>高并发写</h3><ol><li>数据分片<ul><li>数据库分库分表</li><li>JDK concurrentHashMap实现</li><li>kafka的partition</li><li>ES的分布式索引</li></ul></li><li>任务分片<ul><li>CPU的指令流水线</li><li>Map/Reduce</li><li>Tomcat 1+N+M 网络模型：1个监听线程，N个IO线程负责对socket进行读写，M个worker对请求做逻辑处理。</li></ul></li><li>异步化：异步接口、异步IO<ul><li>短信验证码注册/登录</li><li>订单系统</li><li>广告计费系统，异步，多消息合并扣费</li><li>Kafka的Pipeline</li></ul></li><li>WAL技术<ul><li>数据库redo log</li><li>LSM树 </li></ul></li><li>批量<ul><li>kafka的百万qps写入:partition分片，磁盘顺序写入，批量（leader/follower之间的批量，本地client之间的批量）</li><li>mysql的group commit机制，对多事务的redo log批量flush</li></ul></li></ol><h3 id="高并发读"><a href="#高并发读" class="headerlink" title="高并发读"></a>高并发读</h3><ol><li>加缓存<ul><li>本地缓存/redis/memcached</li></ul></li><li>增加副本冗余<ul><li>MySQL master/slave</li><li>CDN 静态文件加速</li></ul></li><li>并发读<ul><li>异步RPC</li><li>冗余请求，降低失败率</li></ul></li></ol><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;高并发写&quot;&gt;&lt;a href=&quot;#高并发写&quot; class=&quot;headerlink&quot; title=&quot;高并发写&quot;&gt;&lt;/a&gt;高并发写&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;数据分片&lt;ul&gt;
&lt;li&gt;数据库分库分表&lt;/li&gt;
&lt;li&gt;JDK concurrentHashMap实现&lt;/l
      
    
    </summary>
    
    
      <category term="分布式" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="方案总结" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%96%B9%E6%A1%88%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="分布式" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>多副本一致性</title>
    <link href="http://yoursite.com/2020/05/12/%E5%A4%9A%E5%89%AF%E6%9C%AC%E4%B8%80%E8%87%B4%E6%80%A7/"/>
    <id>http://yoursite.com/2020/05/12/%E5%A4%9A%E5%89%AF%E6%9C%AC%E4%B8%80%E8%87%B4%E6%80%A7/</id>
    <published>2020-05-12T03:23:33.000Z</published>
    <updated>2020-05-12T05:09:02.394Z</updated>
    
    <content type="html"><![CDATA[<h3 id="同步本质"><a href="#同步本质" class="headerlink" title="同步本质"></a>同步本质</h3><p>每台机器都把收到的请求按日志存下来，各机器的日志文件保持一致。选择存储“事件流”，而非最终状态，原因是：</p><ol><li>日志只有一种操作，append，相对简单</li></ol><h3 id="Paxos算法"><a href="#Paxos算法" class="headerlink" title="Paxos算法"></a>Paxos算法</h3><h4 id="1-Basic-Paxos"><a href="#1-Basic-Paxos" class="headerlink" title="1. Basic Paxos"></a>1. Basic Paxos</h4><ol><li>两个角色，Proposer 和 Acceptor，以及一个自增ID（n）</li><li>两个阶段，Propose阶段 和 Accept 阶段</li><li>Propose阶段<ol><li>proposer广播消息，id为n，prepare(n)</li><li>acceptor接收消息，如果n &gt; local N，则回复yes</li><li>proposer收到半数以上的yes，开始广播，否则id自增，重新广播</li></ol></li><li>Acctpt阶段<ol><li>proposer广播消息, accept(n, value)</li><li>acceptor接收消息，如果n &gt; loacal N，则持久化，返回yes</li><li>proposer收到半数以上的yes，则结束。否则id自增，从proposer阶段重新开始。</li></ol></li><li>两个问题<ol><li>Paxos是个不断循环的2PC，有可能陷入死循环，所谓“活锁”。比如3个node同时propose，都收到no，又同时n++，继续propose，继续no</li><li>性能：每次写入，需要两次RTT + 两次写盘。两次RTT分别是Propose/Accept阶段。这两个阶段都会持久化一些变量，需要磁盘IO。</li></ol></li><li>活锁问题<ol><li>多点写入，变为单点写入。选出一个leader，只让leader当proposer。从而减少冲突。leader选取办法，比如每个节点增加编号，使用心跳，选取编号最大的节点为leader。即使出现同一时间，多个leader，也不影响paxos的正确性，只会增大并发写冲突的概率。</li></ol></li></ol><h3 id="Raft算法"><a href="#Raft算法" class="headerlink" title="Raft算法"></a>Raft算法</h3><ol><li>单点写入：任一时刻，只允许一个有效的leader存在，所有的写请求，都传到leader上，然后由leader同步给超过半数的follower。</li><li>单条日志结构：term + index + content。term是leader的任期，只会单调递增；index是日志顺序编号，也是递增；</li><li>分为三个阶段，选举阶段，正常阶段，恢复阶段</li><li>选举阶段<ol><li>节点有三个状态：leader、follower、candidate。candidate是个中间状态。</li><li>当follower在一定时间收不到leader心跳时，就会随机sleep一个时间，然后变为candidate，发起选举。选举结束后，变为leader或follower。</li><li>选举算法，保证同一时间只有一个leader。<ol><li>如果选举请求里，日志的term和index比自己本地的新，则返回true，否则返回false。</li><li>candidate收到多数派返回true，则成为leader</li><li>每个节点只能投一次true，防止多个leader。因此选取出的leader不一定是最新的，但一定比大多数节点新。</li></ol></li></ol></li><li>正常阶段，复制日志<ol><li>只要超过半数的follower复制成功，就返回给客户端日志写入成功。</li><li>关键的日志一致性保证：<blockquote><ol><li>如果两个节点的日志，index和term相同，则内容一定相同。</li><li>如果index=M处的日志相同，则在M之前的日志，也一定相同。</li></ol></blockquote></li></ol></li><li>恢复阶段<ol><li>leader同步term给follower</li><li>以leader本地的日志为基准，复制给follower</li></ol></li><li>安全性保证<ol><li>leader数据是基准，leader不会从别的节点同步数据，只会是别的节点根据leader数据删除或追加自己的数据。</li><li>对于已经commit的日志，一定是commit的。对于新任leader上，前任leader未commit的日志，稍后会变为commit状态。不在新任leader上的未commit数据，会被覆盖。</li></ol></li></ol><h3 id="Zab"><a href="#Zab" class="headerlink" title="Zab"></a>Zab</h3><p>zookeeper使用的强一致性算法，同时也是单点写入，写请求都转发给leader。</p><ol><li>模型对比，复制状态机(replicated state machine, paxos/raft) vs 主备系统（primay-backup system，zab）,前者持久化的是客户端的请求序列（日志序列），另外一个持久化的是数据的状态变化。<ol><li>数据同步次数不一样，如果client执行三次x=1，后两次在主备系统里，不用触发同步。</li><li>存储状态变化，具有幂等性，而复制状态机不具备。</li></ol></li><li>zxid<ol><li>高32位，leader任期，类似raft的term</li><li>低32位，日志序列，类似raft的日志index</li></ol></li><li>三个阶段：Leader选举，BroadCast,恢复阶段</li><li>Leader选举：FLE算法<ol><li>Leader和Follower之间是双向心跳；raft里是单向</li><li>选取zxid最大的节点作为leader；和raft选取term+index最新的节点作为leader一个意思。</li></ol></li><li>broadcast阶段<ol><li></li></ol></li></ol><h3 id="raft-vs-zab"><a href="#raft-vs-zab" class="headerlink" title="raft vs zab"></a>raft vs zab</h3><p>参考：<a href="https://my.oschina.net/pingpangkuangmo/blog/782702" target="_blank" rel="noopener">https://my.oschina.net/pingpangkuangmo/blog/782702</a></p><ol><li>上一轮残留的数据怎么处理？</li></ol><p>首先看下上一轮次的leader在挂或者失去leader位置之前，会有哪些数据？</p><ul><li>已过半复制的日志</li><li>未过半复制的日志<br>一个日志是否被过半复制，是否被提交，这些信息是由leader才能知晓的，</li></ul><p>那么下一个leader该如何来判定这些日志呢？</p><p>下面分别来看看Raft和ZooKeeper的处理策略：</p><p>Raft：对于之前term的过半或未过半复制的日志采取的是保守的策略，全部判定为未提交，只有当当前term的日志过半了，才会顺便将之前term的日志进行提交</p><p>ZooKeeper：采取激进的策略，对于所有过半还是未过半的日志都判定为提交，都将其应用到状态机中</p><p>Raft的保守策略更多是因为Raft在leader选举完成之后，没有同步更新过程来保持和leader一致（在可以对外服务之前的这一同步过程）。而ZooKeeper是有该过程的</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;同步本质&quot;&gt;&lt;a href=&quot;#同步本质&quot; class=&quot;headerlink&quot; title=&quot;同步本质&quot;&gt;&lt;/a&gt;同步本质&lt;/h3&gt;&lt;p&gt;每台机器都把收到的请求按日志存下来，各机器的日志文件保持一致。选择存储“事件流”，而非最终状态，原因是：&lt;/p&gt;
&lt;ol&gt;

      
    
    </summary>
    
    
      <category term="后端" scheme="http://yoursite.com/categories/%E5%90%8E%E7%AB%AF/"/>
    
      <category term="系统原理" scheme="http://yoursite.com/categories/%E5%90%8E%E7%AB%AF/%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86/"/>
    
    
      <category term="分布式" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>开天辟地</title>
    <link href="http://yoursite.com/2020/05/11/%E5%BC%80%E5%A4%A9%E8%BE%9F%E5%9C%B0/"/>
    <id>http://yoursite.com/2020/05/11/%E5%BC%80%E5%A4%A9%E8%BE%9F%E5%9C%B0/</id>
    <published>2020-05-11T09:05:19.000Z</published>
    <updated>2020-05-14T03:04:18.669Z</updated>
    
    <content type="html"><![CDATA[<p>欢迎</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;欢迎&lt;/p&gt;

      
    
    </summary>
    
    
    
  </entry>
  
</feed>
