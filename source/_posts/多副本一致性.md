---
title: 多副本一致性
date: 2020-05-12 11:23:33
tags: [分布式]
categories: [后端,系统原理]
---
### 同步本质
每台机器都把收到的请求按日志存下来，各机器的日志文件保持一致。选择存储“事件流”，而非最终状态，原因是：
1. 日志只有一种操作，append，相对简单

### Paxos算法
#### 1. Basic Paxos
1. 两个角色，Proposer 和 Acceptor，以及一个自增ID（n）
2. 两个阶段，Propose阶段 和 Accept 阶段
3. Propose阶段
   1. proposer广播消息，id为n，prepare(n)
   2. acceptor接收消息，如果n > local N，则回复yes
   3. proposer收到半数以上的yes，开始广播Accept，否则id自增，重新广播Propose
3. Acctpt阶段
   1. proposer广播消息, accept(n, value)
   2. acceptor接收消息，如果n > loacal N，则持久化，返回yes
   3. proposer收到半数以上的yes，则结束。否则id自增，从proposer阶段重新开始。
5. 两个问题
   1. Paxos是个不断循环的2PC，有可能陷入死循环，所谓“活锁”。比如3个node同时propose，都收到no，又同时n++，继续propose，继续no
   2. 性能：每次写入，需要两次RTT + 两次写盘。两次RTT分别是Propose/Accept阶段。这两个阶段都会持久化一些变量，需要磁盘IO。
6. 活锁问题
   1. 多点写入，变为单点写入。选出一个leader，只让leader当proposer。从而减少冲突。leader选取办法，比如每个节点增加编号，使用心跳，选取编号最大的节点为leader。即使出现同一时间，多个leader，也不影响paxos的正确性，只会增大并发写冲突的概率。

### Raft算法
1. 单点写入：任一时刻，只允许一个有效的leader存在，所有的写请求，都传到leader上，然后由leader同步给超过半数的follower。
2. 单条日志结构：term + index + content。term是leader的任期，只会单调递增；index是日志顺序编号，也是递增；
2. 分为三个阶段，选举阶段，正常阶段，恢复阶段
3. 选举阶段
   1. 节点有三个状态：leader、follower、candidate。candidate是个中间状态。
   2. 当follower在一定时间收不到leader心跳时，就会随机sleep一个时间，然后变为candidate，发起选举。选举结束后，变为leader或follower。
   3. 选举算法，保证同一时间只有一个leader。
      1. 如果选举请求里，日志的term和index比自己本地的新，则返回true，否则返回false。
      2. candidate收到多数派返回true，则成为leader
      3. 每个节点只能投一次true，防止多个leader。**因此选取出的leader不一定是最新的，但一定比大多数节点新。** 
4. 正常阶段，复制日志
   1. 只要超过半数的follower复制成功，就返回给客户端日志写入成功。
   2. 关键的日志一致性保证：
      > 1. 如果两个节点的日志，index和term相同，则内容一定相同。
      > 2. 如果index=M处的日志相同，则在M之前的日志，也一定相同。
5. 恢复阶段
   1. leader同步term给follower
   2. 以leader本地的日志为基准，复制给follower。这里比较特殊，如果新leader本身有未commit的日志，需要跟新的日志一起提交。避免一些特殊情况下，已commit的日志被覆盖。
6. 安全性保证
   1. leader数据是基准，leader不会从别的节点同步数据，只会是别的节点根据leader数据删除或追加自己的数据。
   2. 对于已经commit的日志，一定是commit的。对于新任leader上，前任leader未commit的日志，稍后会变为commit状态。不在新任leader上的未commit数据，会被覆盖。

### Zab
zookeeper使用的强一致性算法，同时也是单点写入，写请求都转发给leader。
1. 模型对比，复制状态机(replicated state machine, paxos/raft) vs 主备系统（primay-backup system，zab）,前者持久化的是客户端的请求序列（日志序列），另外一个持久化的是数据的状态变化。
   1. 数据同步次数不一样，如果client执行三次x=1，后两次在主备系统里，不用触发同步。
   2. 存储状态变化，具有幂等性，而复制状态机不具备。
2. zxid
   1. 高32位，leader任期，类似raft的term
   2. 低32位，日志序列，类似raft的日志index
3. 三个阶段：Leader选举，BroadCast,恢复阶段
4. Leader选举：FLE算法
   1. Leader和Follower之间是双向心跳；raft里是单向
   2. 选取zxid最大的节点作为leader；和raft选取term+index最新的节点作为leader一个意思。
5. broadcast阶段
6. zookeeper有个obverser模式，很有意思。可以在不影响写性能的情况下，提升读性能。主要原理就是：增加一类节点，处于observer状态。这些节点不参与投票，只提供读的功能，相当于额外的follower节点副本。（其实，对于提升读性能，总结也就两个办法：加缓存或加副本冗余。）
   
### raft vs zab
参考：https://my.oschina.net/pingpangkuangmo/blog/782702
1. 上一轮残留的数据怎么处理？

首先看下上一轮次的leader在挂或者失去leader位置之前，会有哪些数据？
- 已过半复制的日志
- 未过半复制的日志
一个日志是否被过半复制，是否被提交，这些信息是由leader才能知晓的，

那么下一个leader该如何来判定这些日志呢？

下面分别来看看Raft和ZooKeeper的处理策略：

Raft：对于之前term的过半或未过半复制的日志采取的是保守的策略，全部判定为未提交，只有当当前term的日志过半了，才会顺便将之前term的日志进行提交。

ZooKeeper：采取激进的策略，对于所有过半还是未过半的日志都判定为提交，都将其应用到状态机中。

Raft的保守策略更多是因为Raft在leader选举完成之后，没有同步更新过程来保持和leader一致（在可以对外服务之前的这一同步过程）。而ZooKeeper是有该过程的。